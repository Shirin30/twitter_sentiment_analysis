{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3306cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\n",
    "test = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing and cleaning of the data\n",
    "# removing twitter handles\n",
    "# combine train and test set\n",
    "combi = train.append(test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# here we replace everything except characters and hashtags with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all words less than or equal to 3\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d370e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we will tokenize all the clean tweets in our dataset. tokenisation is the process of splitting sentences into words and tokens are the individual words.\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x : x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is a process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitching the tokens back together\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5182f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation of a word cloud\n",
    "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting word cloud for non sexist and non racist words\n",
    "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting word cloud for sexist and racist words\n",
    "negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500,\n",
    "random_state=21, max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting hashtags from non racist/sexist tweets\n",
    "\n",
    "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n",
    "\n",
    "# extracting hashtags from racist/sexist tweets\n",
    "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n",
    "\n",
    "# unnesting list\n",
    "HT_regular = sum(HT_regular,[])\n",
    "HT_negative = sum(HT_negative,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba29d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(HT_regular)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "# selecting top 10 most frequent hashtags     \n",
    "d = d.nlargest(columns=\"Count\", n = 10) \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nltk.FreqDist(HT_negative)\n",
    "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
    "# selecting top 10 most frequent hashtags\n",
    "e = e.nlargest(columns=\"Count\", n = 10)   \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word 2 vector\n",
    "import gensim\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x : x.split())\n",
    "model_w2v = gensim.models.Word2Vec(tokenized_tweet, window=5, min_count=2, sg=1, hs=0, negative=10, workers = 2, seed=34)\n",
    "model_w2v.train(tokenized_tweet, total_examples = len(combi['tidy_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens :\n",
    "        try :\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError :\n",
    "            continue\n",
    "    if count!=0 :\n",
    "        vec/=count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 100))\n",
    "for i in range(len(tokenized_tweet)) :\n",
    "    wordvec_arrays[i, :] = word_vector(tokenized_tweet[i], 100)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model using bag of words features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "train_bow = bow[:31962,:]\n",
    "test_bow = bow[31962:,:]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain_bow, ytrain) # training the model\n",
    "\n",
    "# y_pred = lreg.predict(xvalid_bow)\n",
    "# acc = accuracy_score(yvalid, y_pred)\n",
    "# print(acc)\n",
    "# y_pred = lreg.predict(xvalid_bow)\n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(lreg.score(xvalid_bow, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(test_bow)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_logisticreg_bow.csv', index=False) # writing data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model using tfidf features\n",
    "train_tfidf = tfidf[:31962,:]\n",
    "test_tfidf = tfidf[31962:,:]\n",
    "\n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "lreg.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(lreg.score(xvalid_tfidf, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(test_tfidf)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_logisticreg_tfidf.csv', index=False) # writing data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression using word 2 vec\n",
    "train_w2v = wordvec_df.iloc[:31962, :]\n",
    "test_w2v = wordvec_df.iloc[31962:, :]\n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
    "\n",
    "lreg.fit(xtrain_w2v, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_w2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int))\n",
    "print(lreg.score(xvalid_w2v, yvalid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using support vector machines\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ebb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building svm model for bow\n",
    "svc = svm.SVC(kernel = 'linear', C=1, probability=True).fit(xtrain_bow, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_bow)\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(svc.score(xvalid_bow, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ffbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = svc.predict_proba(test_bow)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_svm_bow.csv', index=False) # writing data to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc16370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building svm model for tfidf\n",
    "svc = svm.SVC(kernel = 'linear', C=1, probability=True).fit(xtrain_tfidf, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(svc.score(xvalid_tfidf, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = svc.predict_proba(test_tfidf)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_svm_tfidf.csv', index=False) # writing data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building svm model for word2vec features\n",
    "svc = svm.SVC(kernel = 'linear', C=1, probability=True).fit(xtrain_w2v, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_w2v)\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(svc.score(xvalid_w2v, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e178ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building model using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac37586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building random forest model using bow features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain)\n",
    "prediction = rf.predict(xvalid_bow)\n",
    "print(f1_score(yvalid, prediction)) # calculating f1 score\n",
    "print(rf.score(xvalid_bow, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51195c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building random forest model using tfidf features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain)\n",
    "prediction = rf.predict(xvalid_tfidf)\n",
    "print(f1_score(yvalid, prediction)) # calculating f1 score\n",
    "print(rf.score(xvalid_tfidf, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6618ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building random forest model using word2vec features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain)\n",
    "prediction = rf.predict(xvalid_w2v)\n",
    "print(f1_score(yvalid, prediction)) # calculating f1 score\n",
    "print(rf.score(xvalid_w2v, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model using XG boost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building xgb model using bow\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\n",
    "prediction = xgb_model.predict(xvalid_bow)\n",
    "print(f1_score(prediction, yvalid)) # calculating f1 score\n",
    "print(xgb_model.score(xvalid_bow, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4934268",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = xgb_model.predict(test_bow)\n",
    "test['label'] = test_pred\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_xgb_bow.csv', index=False) # writing data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafa554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building xgb model using tfidf features\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)\n",
    "prediction = xgb_model.predict(xvalid_tfidf)\n",
    "print(f1_score(prediction, yvalid)) # calculating f1 score\n",
    "print(xgb_model.score(xvalid_tfidf, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building xgb model using tfidf features\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_w2v, ytrain)\n",
    "prediction = xgb_model.predict(xvalid_w2v)\n",
    "print(f1_score(prediction, yvalid)) # calculating f1 score\n",
    "print(xgb_model.score(xvalid_w2v, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91886bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost with fine tuning\n",
    "# here we will use D-Matrices. A D-Matrix can use both features and target\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain)\n",
    "dvalid = xgb.DMatrix(xvalid_w2v, label=yvalid)\n",
    "dtest = xgb.DMatrix(test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':6,\n",
    "    'min_child_weight':1,\n",
    "    'eta':.3,\n",
    "    'subsample':1,\n",
    "    'colsample_bytree':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047126dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label().astype(np.int)\n",
    "    preds = (preds>=0.3).astype(np.int)\n",
    "    return [('f1_score', f1_score(labels, preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5782862",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range (6,10)\n",
    "        for min_child_weight in range (5,8)\n",
    "]\n",
    "max_f1 = 0 #initializing with 0\n",
    "best_params = None\n",
    "for max_depth , min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(max_depth, min_child_weight))\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    \n",
    "    # cross validation\n",
    "    cv_results = xgb.cv(params, dtrain, feval=custom_eval, num_boost_round=200, maximize=True, seed=16, nfold=5, early_stopping_rounds=10)\n",
    "\n",
    "# finding best f1-score\n",
    "mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "print(\"\\t f1 score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "if mean_f1>max_f1:\n",
    "    max_f1 = mean_f1\n",
    "    best_params = (max_depth, min_child_weight)\n",
    "print(\"Best params: {} {} f1 score:{} \".format(best_params[0], best_params[1], max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = 9\n",
    "params['min_child_weight'] = 7\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(5,10)]\n",
    "    for colsample in [i/10. for i in range(5,10)] ]\n",
    "max_f1 = 0 #initializing with 0\n",
    "best_params = None\n",
    "for subsample, colsample in gridsearch_params:\n",
    "    print(\"CV with subsample={}, colsample={}\".format(subsample, colsample))\n",
    "    params['colsample'] = colsample\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    # cross validation\n",
    "    cv_results = xgb.cv(params, dtrain, feval=custom_eval, num_boost_round=200, maximize=True, seed=16, nfold=5, early_stopping_rounds=10)\n",
    "\n",
    "# finding best f1-score\n",
    "mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "print(\"\\t f1 score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "if mean_f1>max_f1:\n",
    "    max_f1 = mean_f1\n",
    "    best_params = (subsample, colsample)\n",
    "print(\"Best params: {} {} f1 score:{} \".format(best_params[0], best_params[1], max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['subsample'] = .9\n",
    "params['colsample_bytree'] = .9\n",
    "max_f1 = 0\n",
    "best_params = None\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    # update eta\n",
    "    params['eta'] = eta\n",
    "    \n",
    "    \n",
    "    # run cv\n",
    "    # cross validation\n",
    "    cv_results = xgb.cv(params, dtrain, feval=custom_eval, num_boost_round=1000, maximize=True, seed=16, nfold=5, early_stopping_rounds=20)\n",
    "    \n",
    "# finding best f1-score\n",
    "mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "print(\"\\t f1 score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "if mean_f1>max_f1:\n",
    "    max_f1 = mean_f1\n",
    "    best_params = eta\n",
    "print(\"Best params: {} f1 score:{} \".format(best_params, max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753accf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':9,\n",
    "    'min_child_weight':7,\n",
    "    'eta':.1,\n",
    "    'subsample':.9,\n",
    "    'colsample_bytree':.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac22cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.train(params, dtrain, feval=custom_eval,num_boost_round=1000, maximize=True, evals=[(dvalid, \"Validation\")], early_stopping_rounds=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
