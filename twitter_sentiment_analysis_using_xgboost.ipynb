{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\r\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\r\n",
    "# For example, here's several helpful packages to load\r\n",
    "\r\n",
    "import numpy as np # linear algebra\r\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "\r\n",
    "# Input data files are available in the read-only \"../input/\" directory\r\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\r\n",
    "\r\n",
    "import os\r\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\r\n",
    "    for filename in filenames:\r\n",
    "        print(os.path.join(dirname, filename))\r\n",
    "\r\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \r\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import seaborn as sns\r\n",
    "import string\r\n",
    "import nltk\r\n",
    "import warnings \r\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\r\n",
    "test = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# preprocessing and cleaning of the data\r\n",
    "# removing twitter handles\r\n",
    "# combine train and test set\r\n",
    "combi = train.append(test, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_pattern(input_txt, pattern):\r\n",
    "    r = re.findall(pattern, input_txt)\r\n",
    "    for i in r:\r\n",
    "        input_txt = re.sub(i, '', input_txt)\r\n",
    "        \r\n",
    "    return input_txt    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove twitter handles (@user)\r\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove special characters, numbers, punctuations\r\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\r\n",
    "# here we replace everything except characters and hashtags with spaces"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove all words less than or equal to 3\r\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "combi.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now, we will tokenize all the clean tweets in our dataset. tokenisation is the process of splitting sentences into words and tokens are the individual words.\r\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x : x.split())\r\n",
    "tokenized_tweet.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Stemming is a process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\r\n",
    "from nltk.stem.porter import *\r\n",
    "stemmer = PorterStemmer()\r\n",
    "\r\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\r\n",
    "tokenized_tweet.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# stitching the tokens back together\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "combi.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# formation of a word cloud\n",
    "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plotting word cloud for non sexist and non racist words\n",
    "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plotting word cloud for sexist and racist words\n",
    "negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500,\n",
    "random_state=21, max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# extracting hashtags from non racist/sexist tweets\n",
    "\n",
    "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n",
    "\n",
    "# extracting hashtags from racist/sexist tweets\n",
    "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n",
    "\n",
    "# unnesting list\n",
    "HT_regular = sum(HT_regular,[])\n",
    "HT_negative = sum(HT_negative,[])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = nltk.FreqDist(HT_regular)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "# selecting top 10 most frequent hashtags     \n",
    "d = d.nlargest(columns=\"Count\", n = 10) \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b = nltk.FreqDist(HT_negative)\n",
    "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
    "# selecting top 10 most frequent hashtags\n",
    "e = e.nlargest(columns=\"Count\", n = 10)   \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#word 2 vector\r\n",
    "import gensim\r\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x : x.split())\r\n",
    "model_w2v = gensim.models.Word2Vec(tokenized_tweet, window=5, min_count=2, sg=1, hs=0, negative=10, workers = 2, seed=34)\r\n",
    "model_w2v.train(tokenized_tweet, total_examples = len(combi['tidy_tweet']), epochs=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def word_vector(tokens, size):\r\n",
    "    vec = np.zeros(size).reshape((1, size))\r\n",
    "    count = 0\r\n",
    "    for word in tokens :\r\n",
    "        try :\r\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\r\n",
    "            count += 1\r\n",
    "        except KeyError :\r\n",
    "            continue\r\n",
    "    if count!=0 :\r\n",
    "        vec/=count\r\n",
    "    return vec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 100))\r\n",
    "for i in range(len(tokenized_tweet)) :\r\n",
    "    wordvec_arrays[i, :] = word_vector(tokenized_tweet[i], 100)\r\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\r\n",
    "wordvec_df.shape\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# logistic regression model using bag of words features\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import f1_score, accuracy_score\r\n",
    "\r\n",
    "train_bow = bow[:31962,:]\r\n",
    "test_bow = bow[31962:,:]\r\n",
    "\r\n",
    "# splitting data into training and validation set\r\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\r\n",
    "\r\n",
    "lreg = LogisticRegression()\r\n",
    "lreg.fit(xtrain_bow, ytrain) # training the model\r\n",
    "\r\n",
    "# y_pred = lreg.predict(xvalid_bow)\r\n",
    "# acc = accuracy_score(yvalid, y_pred)\r\n",
    "# print(acc)\r\n",
    "# y_pred = lreg.predict(xvalid_bow)\r\n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\r\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\r\n",
    "prediction_int = prediction_int.astype(np.int)\r\n",
    "\r\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\r\n",
    "print(lreg.score(xvalid_bow, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_pred = lreg.predict_proba(test_bow)\r\n",
    "test_pred_int = test_pred[:,1] >= 0.3\r\n",
    "test_pred_int = test_pred_int.astype(np.int)\r\n",
    "test['label'] = test_pred_int\r\n",
    "submission = test[['id','label']]\r\n",
    "submission.to_csv('sub_logisticreg_bow.csv', index=False) # writing data to a CSV file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# logistic regression model using tfidf features\r\n",
    "train_tfidf = tfidf[:31962,:]\r\n",
    "test_tfidf = tfidf[31962:,:]\r\n",
    "\r\n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\r\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\r\n",
    "\r\n",
    "lreg.fit(xtrain_tfidf, ytrain)\r\n",
    "\r\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)\r\n",
    "prediction_int = prediction[:,1] >= 0.3\r\n",
    "prediction_int = prediction_int.astype(np.int)\r\n",
    "\r\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\r\n",
    "print(lreg.score(xvalid_tfidf, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_pred = lreg.predict_proba(test_tfidf)\r\n",
    "test_pred_int = test_pred[:,1] >= 0.3\r\n",
    "test_pred_int = test_pred_int.astype(np.int)\r\n",
    "test['label'] = test_pred_int\r\n",
    "submission = test[['id','label']]\r\n",
    "submission.to_csv('sub_logisticreg_tfidf.csv', index=False) # writing data to a CSV file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# logistic regression using word 2 vec\n",
    "train_w2v = wordvec_df.iloc[:31962, :]\n",
    "test_w2v = wordvec_df.iloc[31962:, :]\n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
    "\n",
    "lreg.fit(xtrain_w2v, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_w2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int))\n",
    "print(lreg.score(xvalid_w2v, yvalid))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# using support vector machines\n",
    "from sklearn import svm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building svm model for bow\n",
    "svc = svm.SVC(kernel = 'linear', C=1, probability=True).fit(xtrain_bow, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_bow)\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(svc.score(xvalid_bow, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_pred = svc.predict_proba(test_bow)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_svm_bow.csv', index=False) # writing data to a CSV file\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building svm model for tfidf\n",
    "svc = svm.SVC(kernel = 'linear', C=1, probability=True).fit(xtrain_tfidf, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(svc.score(xvalid_tfidf, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_pred = svc.predict_proba(test_tfidf)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_svm_tfidf.csv', index=False) # writing data to a CSV file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building svm model for word2vec features\n",
    "svc = svm.SVC(kernel = 'linear', C=1, probability=True).fit(xtrain_w2v, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_w2v)\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(svc.score(xvalid_w2v, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#building model using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building random forest model using bow features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain)\n",
    "prediction = rf.predict(xvalid_bow)\n",
    "print(f1_score(yvalid, prediction)) # calculating f1 score\n",
    "print(rf.score(xvalid_bow, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building random forest model using tfidf features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain)\n",
    "prediction = rf.predict(xvalid_tfidf)\n",
    "print(f1_score(yvalid, prediction)) # calculating f1 score\n",
    "print(rf.score(xvalid_tfidf, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building random forest model using word2vec features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain)\n",
    "prediction = rf.predict(xvalid_w2v)\n",
    "print(f1_score(yvalid, prediction)) # calculating f1 score\n",
    "print(rf.score(xvalid_w2v, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building model using XG boost\n",
    "from xgboost import XGBClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building xgb model using bow\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\n",
    "prediction = xgb_model.predict(xvalid_bow)\n",
    "print(f1_score(prediction, yvalid)) # calculating f1 score\n",
    "print(xgb_model.score(xvalid_bow, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_pred = xgb_model.predict(test_bow)\r\n",
    "test['label'] = test_pred\r\n",
    "submission = test[['id','label']]\r\n",
    "submission.to_csv('sub_xgb_bow.csv', index=False) # writing data to a CSV file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building xgb model using tfidf features\r\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)\r\n",
    "prediction = xgb_model.predict(xvalid_tfidf)\r\n",
    "print(f1_score(prediction, yvalid)) # calculating f1 score\r\n",
    "print(xgb_model.score(xvalid_tfidf, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building xgb model using word 2 vec features\r\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_w2v, ytrain)\r\n",
    "prediction = xgb_model.predict(xvalid_w2v)\r\n",
    "print(f1_score(prediction, yvalid)) # calculating f1 score\r\n",
    "print(xgb_model.score(xvalid_w2v, yvalid)) # calculating score"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}