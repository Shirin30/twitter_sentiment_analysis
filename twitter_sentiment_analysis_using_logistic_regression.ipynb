{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7671b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7cafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\n",
    "test = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing and cleaning of the data\n",
    "# removing twitter handles\n",
    "# combine train and test set\n",
    "combi = train.append(test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# here we replace everything except characters and hashtags with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faba37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all words less than or equal to 3\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07307464",
   "metadata": {},
   "outputs": [],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3eae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we will tokenize all the clean tweets in our dataset. tokenisation is the process of splitting sentences into words and tokens are the individual words.\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x : x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is a process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitching the tokens back together\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fd8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation of a word cloud\n",
    "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174bc28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting word cloud for non sexist and non racist words\n",
    "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting word cloud for sexist and racist words\n",
    "negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500,\n",
    "random_state=21, max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63569d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting hashtags from non racist/sexist tweets\n",
    "\n",
    "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n",
    "\n",
    "# extracting hashtags from racist/sexist tweets\n",
    "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n",
    "\n",
    "# unnesting list\n",
    "HT_regular = sum(HT_regular,[])\n",
    "HT_negative = sum(HT_negative,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59728a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(HT_regular)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "# selecting top 10 most frequent hashtags     \n",
    "d = d.nlargest(columns=\"Count\", n = 10) \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e27477",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nltk.FreqDist(HT_negative)\n",
    "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
    "# selecting top 10 most frequent hashtags\n",
    "e = e.nlargest(columns=\"Count\", n = 10)   \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bcc91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word 2 vector\n",
    "import gensim\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x : x.split())\n",
    "model_w2v = gensim.models.Word2Vec(tokenized_tweet, window=5, min_count=2, sg=1, hs=0, negative=10, workers = 2, seed=34)\n",
    "model_w2v.train(tokenized_tweet, total_examples = len(combi['tidy_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fbbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens :\n",
    "        try :\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError :\n",
    "            continue\n",
    "    if count!=0 :\n",
    "        vec/=count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 100))\n",
    "for i in range(len(tokenized_tweet)) :\n",
    "    wordvec_arrays[i, :] = word_vector(tokenized_tweet[i], 100)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model using bag of words features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "train_bow = bow[:31962,:]\n",
    "test_bow = bow[31962:,:]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain_bow, ytrain) # training the model\n",
    "\n",
    "# y_pred = lreg.predict(xvalid_bow)\n",
    "# acc = accuracy_score(yvalid, y_pred)\n",
    "# print(acc)\n",
    "# y_pred = lreg.predict(xvalid_bow)\n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(lreg.score(xvalid_bow, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10218607",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(test_bow)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_logisticreg_bow.csv', index=False) # writing data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1614091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model using tfidf features\n",
    "train_tfidf = tfidf[:31962,:]\n",
    "test_tfidf = tfidf[31962:,:]\n",
    "\n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "lreg.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int)) # calculating f1 score\n",
    "print(lreg.score(xvalid_tfidf, yvalid)) # calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(test_tfidf)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_logisticreg_tfidf.csv', index=False) # writing data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1506535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression using word 2 vec\n",
    "train_w2v = wordvec_df.iloc[:31962, :]\n",
    "test_w2v = wordvec_df.iloc[31962:, :]\n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
    "\n",
    "lreg.fit(xtrain_w2v, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_w2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(yvalid, prediction_int))\n",
    "print(lreg.score(xvalid_w2v, yvalid))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
